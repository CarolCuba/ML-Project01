{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression: Implemented Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "\n",
    "#Creating the functions used for Linear Regression\n",
    "\n",
    "'''Criando a funcao de normalização de um dataframe inteiro\n",
    "    input:\n",
    "        df: Dataframe\n",
    "    output:\n",
    "        df: Dataframe com valores normalizados\n",
    "'''\n",
    "def normalize_dataframe(df):\n",
    "    for column in df:\n",
    "        df[column] = (df[column] - min(df[column])) / ( max(df[column]) - min(df[column]) )\n",
    "    return df\n",
    "\n",
    "'''Criando a funcao de normalização de uma coluna do dataframe\n",
    "    input:\n",
    "        df: Coluna a ser normalizada de um dataframe\n",
    "    output:\n",
    "        coluna: Coluna do Dataframe normalizada\n",
    "'''\n",
    "def normalize_dataframe_column(df):\n",
    "        return (df - min(df)) / ( max(df) - min(df) )\n",
    "\n",
    "'''Criando funcao para executar o gradiente descendente (batch)\n",
    "    input:\n",
    "        X: matriz de features\n",
    "        Y: matriz de gabarito\n",
    "        theta: array de pesos\n",
    "        alpha:learning rate\n",
    "        iterations: numero max de iteracoes para parada\n",
    "    output:\n",
    "       theta: vetor de thetas atualizados\n",
    "       cost_history: vetor do historico de custos ao longo do processo\n",
    "'''\n",
    "def stochastic_gradient_descent(X,Y,theta,alpha,iterations):\n",
    "    qut_samples = Y.shape[0]\n",
    "    cost_history = [0]*iterations*qut_samples\n",
    "    \n",
    "    for iteration in range (iterations):\n",
    "        for sample in range (qut_samples):#hipotese\n",
    "            h = X.iloc[sample,:].dot(theta)\n",
    "            #diferenca entre hipotese e predicao\n",
    "            loss = h - Y[sample]\n",
    "            gradient = X.iloc[sample,:].T * loss\n",
    "            #changing values of parameters \n",
    "            theta = theta - np.array([(alpha * gradient)]).T\n",
    "            #New cost value\n",
    "            cost = cost_function(X,Y,theta)\n",
    "            cost_history[iteration*qut_samples+sample] = cost\n",
    "    return theta, cost_history\n",
    "\n",
    "\n",
    "'''Criando funcao para executar o gradiente descendente (estocástico)\n",
    "    input:\n",
    "        X: matriz de features\n",
    "        Y: matriz de gabarito\n",
    "        theta: array de pesos\n",
    "        alpha:learning rate\n",
    "        iterations: numero max de iteracoes para parada\n",
    "        reg_lambda: constante lambda para regularização\n",
    "    output:\n",
    "       theta: vetor de thetas atualizados\n",
    "       cost_history: vetor do historico de custos ao longo do processo\n",
    "'''\n",
    "def stochastic_gradient_descent_regularized(X,Y,theta,alpha,iterations, reg_lambda):\n",
    "    epsulon = 0.00001\n",
    "    qut_samples = Y.shape[0]\n",
    "    cost_history = [0]*iterations*qut_samples\n",
    "    i = 0\n",
    "    \n",
    "    for iteration in range (iterations):\n",
    "        index_aux = iteration*qut_samples\n",
    "        for sample in range (qut_samples):#hipotese\n",
    "            h = X.iloc[sample,:].dot(theta)\n",
    "            #diferenca entre hipotese e predicao\n",
    "            loss = h - Y[sample]\n",
    "            gradient = X.iloc[sample,:].T * loss\n",
    "            #Gradiente descendente\n",
    "            #gradient = np.array(loss * X.iloc[:,sample].T).reshape(1, theta.shape[1])\n",
    "            #changing values of parameters \n",
    "            theta = theta * (1-reg_lambda/qut_samples) - np.array([(alpha * gradient)]).T\n",
    "            #New cost value\n",
    "            cost = cost_function(X,Y,theta)\n",
    "            if(abs(float(cost_history[index_aux+sample-1]-cost)) < epsulon):\n",
    "                i = index_aux+sample\n",
    "                break\n",
    "            cost_history[iteration*qut_samples+sample] = cost\n",
    "        if (i != 0):\n",
    "            break\n",
    "    return theta, cost_history, i\n",
    "\n",
    "\n",
    "\n",
    "'''Criando a funcao de custo\n",
    "    input:\n",
    "        X: matriz de features\n",
    "        Y: matriz de gabarito\n",
    "        theta: array de pesos\n",
    "    output:\n",
    "        custo: custo da iteração\n",
    "'''\n",
    "def cost_function(X,Y,theta):\n",
    "    qut_samples = Y.shape[0]\n",
    "    h = X.dot(theta)\n",
    "    cost = np.sum((h - Y)**2)/(2*qut_samples)\n",
    "    return cost\n",
    "\n",
    "'''Criando funcao para executar o gradiente descendente (batch)\n",
    "    input:\n",
    "        X: matriz de features\n",
    "        Y: matriz de gabarito\n",
    "        theta: array de pesos\n",
    "        alpha:learning rate\n",
    "        iterations: numero max de iteracoes para parada\n",
    "    output:\n",
    "       theta: vetor de thetas atualizados\n",
    "       cost_history: vetor do historico de custos ao longo do processo\n",
    "'''\n",
    "def gradient_descent(X,Y,theta,alpha,iterations):\n",
    "    cost_history = [0]*iterations\n",
    "    qut_samples = Y.shape[0]\n",
    "    epsulon = 0.00001\n",
    "    \n",
    "    for iteration in range (iterations):\n",
    "        #hipotese\n",
    "        h = X.dot(theta)\n",
    "        #diferenca entre hipotese e target\n",
    "        loss = np.array(h - Y).T\n",
    "        #Gradiente descendente\n",
    "        XLoss = X.T.dot(loss.T)\n",
    "        \n",
    "        gradient = XLoss/qut_samples\n",
    "        #changing values of parameters\n",
    "        theta = theta - (alpha * gradient)\n",
    "        #New cost value\n",
    "        cost = cost_function(X,Y,theta)\n",
    "        if(abs(float(cost_history[iteration-1]-cost)) < epsulon):\n",
    "            i = iteration\n",
    "            break\n",
    "        cost_history[iteration] = cost\n",
    "        #print (\"Iteration: %d | Cost: %f\" % (iteration+1, cost))\n",
    "    return theta, cost_history\n",
    "\n",
    "'''Criando funcao para executar o gradiente descendente (batch)\n",
    "    input:\n",
    "        X: matriz de features\n",
    "        Y: matriz de gabarito\n",
    "        theta: array de pesos\n",
    "        alpha:learning rate\n",
    "        iterations: numero max de iteracoes para parada\n",
    "        reg_lambda: constante lambda para regularização\n",
    "    output:\n",
    "       theta: vetor de thetas atualizados\n",
    "       cost_history: vetor do historico de custos ao longo do processo\n",
    "'''\n",
    "def gradient_descent_regularized(X,Y,theta,alpha,iterations, reg_lambda):\n",
    "    cost_history = [0]*iterations\n",
    "    qut_samples = Y.shape[0]\n",
    "    epsulon = 0.00001\n",
    "    \n",
    "    for iteration in range (iterations):\n",
    "        #hipotese\n",
    "        h = X.dot(theta)\n",
    "        #diferenca entre hipotese e target\n",
    "        loss = np.array(h - Y).T\n",
    "        #Gradiente descendente\n",
    "        XLoss = X.T.dot(loss.T)\n",
    "        \n",
    "        gradient = XLoss/qut_samples\n",
    "        #changing values of parameters\n",
    "        theta = theta * (1-reg_lambda/qut_samples) - (alpha * gradient)\n",
    "        #New cost value\n",
    "        cost = cost_function(X,Y,theta)\n",
    "        if(abs(float(cost_history[iteration-1]-cost)) < epsulon):\n",
    "            i = iteration\n",
    "            break\n",
    "        cost_history[iteration] = cost\n",
    "        #print (\"Iteration: %d | Cost: %f\" % (iteration+1, cost))\n",
    "    return theta, cost_history[0:i], i\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Funcao para calcular regressao linear utilizando sklearn \n",
    "    Input:\n",
    "        X: matriz de features\n",
    "        Y: matriz de gabarito\n",
    "        alpha: learning rate\n",
    "        iterations: numero de iteracoes \n",
    "    Output:\n",
    "        rmse: valor de erro gerado na predicao\n",
    "\n",
    "\n",
    "'''\n",
    "def baseLine_GD(X,Y,alpha,iterations):\n",
    "    #Y = Y.T\n",
    "    #X = X.T\n",
    "    reg = linear_model.LinearRegression()\n",
    "    reg.fit(X,Y)\n",
    "    Y_pred = reg.predict(X)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(Y, Y_pred))\n",
    "    return rmse\n",
    "\n",
    "'''Criando funcao para descobrir o erro root-mean-square \n",
    "    input:\n",
    "        Y: vetor gabarito\n",
    "        Y_pred: vetor de predicao gerado pelo modelo\n",
    "    output:\n",
    "        rmse: erro gerado\n",
    "'''\n",
    "def rmse(Y,Y_pred):\n",
    "    rmse = np.sqrt(((Y - Y_pred) ** 2).mean())\n",
    "    return float(rmse)\n",
    "\n",
    "'''Criando funcao para cencontrar valores otimos de theta, usando equacoes normais. \n",
    "    input:\n",
    "        Y: vetor gabarito\n",
    "        X: matriz de features\n",
    "        theta: vetor inicial de pesos\n",
    "    output:\n",
    "        theta: novos valores para os vetor de pesos\n",
    "'''\n",
    "def normal_equation(X,Y,theta):\n",
    "    theta_ = theta.T\n",
    "   \n",
    "    primeiro = X.T.dot(X)\n",
    "    segundo = np.linalg.pinv(primeiro)\n",
    "    terceiro = segundo.dot(X.T)\n",
    "    quarto = terceiro.dot(Y)\n",
    "    theta = quarto\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries creation for discretizing the categorical columns of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionaries for discretizing the categorical columns\n",
    "\n",
    "#cut is a dictionary that uses the semantics of cut feature to create a discrete numerical value based on an e^x function\n",
    "cut = {'Fair':1, 'Good':2, 'Very Good':3, 'Premium':4, 'Ideal':5}\n",
    "color = {'J':1, 'I':2, 'H':3, 'G':4, 'F':5, 'E':6, 'D':7}\n",
    "clarity = {'I1':1, 'SI2':2, 'SI1':3, 'VS2':4, 'VS1':5, 'VVS2':6, 'VVS1':7, 'IF':8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final version of the Dataframe. With normalization, shuffling and outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize and Discretize the DF\n",
    "df = pd.read_csv('../data/diamonds.csv')\n",
    "df = df.sample(frac=1)\n",
    "df[\"cut\"] = df[\"cut\"].map(cut)\n",
    "df[\"color\"] = df[\"color\"].map(color)\n",
    "df[\"clarity\"] = df[\"clarity\"].map(clarity)\n",
    "df['bias'] = 1\n",
    "df['depth'] = normalize_dataframe_column(df['depth'])\n",
    "df['clarity'] = normalize_dataframe_column(df['clarity'])\n",
    "df['color'] = normalize_dataframe_column(df['color'])\n",
    "df['cut'] = normalize_dataframe_column(df['cut'])\n",
    "df['table'] = normalize_dataframe_column(df['table'])\n",
    "#Volume is the combination of all three dimensions of the diamond\n",
    "df['volume'] = df['x'] * df['y'] * df['z']\n",
    "df['volume'] = normalize_dataframe_column(df['volume'])\n",
    "\n",
    "#Separate the DF into Train and Test\n",
    "msk = np.random.rand(len(df)) < 0.85\n",
    "\n",
    "test_df = df[~msk]\n",
    "\n",
    "df = df[msk]\n",
    "\n",
    "#DF is now the training set\n",
    "\n",
    "#Separate the Training DF into Train and Validation\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "\n",
    "train_df = df[msk]\n",
    "validation_df = df[~msk]\n",
    "\n",
    "#Removing outliers\n",
    "\n",
    "#There were diamonds with volume 0 or a super high value\n",
    "train_df = train_df[np.abs(train_df.volume) <= (train_df.volume.mean() + 5 * train_df.volume.std())]\n",
    "train_df = train_df[np.abs(train_df.volume) >= (train_df.volume.mean() -1.5 * train_df.volume.std())]\n",
    "# keep only the ones that are within +5 to -1.5 standard deviations in the column 'volume'.\n",
    "\n",
    "#There were diamonds with depth 0 or a super high value\n",
    "train_df = train_df[np.abs(train_df.depth) <= (train_df.depth.mean() + 5 * train_df.depth.std())]\n",
    "train_df = train_df[np.abs(train_df.depth) >= (train_df.depth.mean() - 7 * train_df.depth.std())]\n",
    "\n",
    "#There were diamonds with table 0 or a super high value\n",
    "train_df = train_df[np.abs(train_df.table) <= (train_df.table.mean() + 6 * train_df.table.std())]\n",
    "train_df = train_df[np.abs(train_df.table) >= (train_df.table.mean() - 5 * train_df.table.std())]\n",
    "\n",
    "\n",
    "Y_train = np.array([train_df['price']]).T\n",
    "Y_validation = np.array([validation_df['price']]).T\n",
    "Y_test = np.array([test_df['price']]).T\n",
    "\n",
    "#Add the Bias and Remove the Target\n",
    "train_df = train_df[['bias','carat', 'cut', 'color','clarity','depth','table','volume']]\n",
    "validation_df = validation_df[['bias','carat', 'cut', 'color','clarity','depth','table','volume']]\n",
    "test_df = test_df[['bias','carat', 'cut', 'color','clarity','depth','table','volume']]\n",
    "\n",
    "theta = np.ones((8,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executions of all implementations with the final dataset for RMSE and Time Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent and Normal Equation Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "iterations = 100000\n",
    "reg_lambda = 10\n",
    "\n",
    "#Batch Gradient Descent\n",
    "start_time_custom = time.time()\n",
    "newTheta,cost_history, iterations_batch = gradient_descent_regularized(train_df,Y_train,theta, alpha, iterations, reg_lambda)\n",
    "Y_pred = validation_df.dot(newTheta)\n",
    "batch_gd_rmse = rmse(Y_validation,Y_pred)\n",
    "batch_gd_time = time.time() - start_time_custom\n",
    "\n",
    "\n",
    "#Normal Distribution\n",
    "start_time_custom = time.time()\n",
    "newTheta_norm = normal_equation(train_df,Y_train,theta)\n",
    "Y_pred_norm = validation_df.dot(newTheta_norm)\n",
    "rmse_norm = rmse(Y_validation,Y_pred_norm)\n",
    "custom_normal_time = time.time() - start_time_custom\n",
    "\n",
    "print(\"RMSE error Batch GD: \",batch_gd_rmse)\n",
    "print(\"RMSE error Norm: \", rmse_norm)\n",
    "\n",
    "#Ploting the data\n",
    "plt.plot(np.arange(iterations_batch), cost_history, label = \"Cost History\")\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('x - axis: Number of iterations')\n",
    "# naming the y axis\n",
    "plt.ylabel('y - axis: Cost')\n",
    "plt.legend()\n",
    " \n",
    "# giving a title to my graph\n",
    "plt.title('Batch Gradient Descent CostxIterations')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (Custom Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "iterations = 5\n",
    "reg_lambda = 10\n",
    "\n",
    "#Stochastic Gradient Descent\n",
    "start_time_custom = time.time()\n",
    "newTheta,cost_history, iterations_stochastic = stochastic_gradient_descent_regularized(train_df,Y_train,theta, alpha, iterations, reg_lambda)\n",
    "Y_pred = validation_df.dot(newTheta)\n",
    "custom_stochastic_gd_rmse = rmse(Y_validation,Y_pred)\n",
    "custom_stochastic_gd_time = time.time() - start_time_custom\n",
    "\n",
    "print(\"RMSE error: \",custom_stochastic_gd_rmse)\n",
    "\n",
    "#Ploting the data\n",
    "plt.plot(np.arange(iterations_stochastic), cost_history, label = \"Cost History\")\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('x - axis: Number of iterations')\n",
    "# naming the y axis\n",
    "plt.ylabel('y - axis: Cost')\n",
    "plt.legend()\n",
    " \n",
    "# giving a title to my graph\n",
    "plt.title('Stochastic Gradient Descent CostxIterations')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (Scikit Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikit SGD\n",
    "start_time_custom = time.time()\n",
    "clf = SGDClassifier(loss=\"squared_loss\", penalty=\"none\", max_iter=5, )\n",
    "clf.fit(train_df.T, Y_train.T)\n",
    "Y_pred = clf.predict(validation_df.T)\n",
    "scikit_stochastic_gd_rmse = rmse(Y_validation.T,Y_pred)\n",
    "scikit_stochastic_gd_time = time.time() - start_time_custom\n",
    "\n",
    "print(\"RMSE error: \", scikit_stochastic_gd_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Gradient Descent Time (10000 iterations):  153.87140226364136\n",
      "Custom Stochastic Gradient Descent Time (5 iteration):  372.1546084880829\n",
      "Custom Normal Equation Time:  0.004178524017333984\n"
     ]
    }
   ],
   "source": [
    "print(\"Batch Gradient Descent Time (\",iterations_batch\" iterations): \", batch_gd_time)\n",
    "print(\"Custom Stochastic Gradient Descent Time (\",iterations_stochastic\" iterations): \", custom_stochastic_gd_time)\n",
    "#print(\"Scikit Stochastic Gradient Descent Time (5 iteration): \", scikit_stochastic_gd_time)\n",
    "print(\"Custom Normal Equation Time: \", custom_normal_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
